{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports (unchanged)\n",
    "import os, re, pickle, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# %% CONFIG (unchanged)\n",
    "MODEL_DIR = \"/Users/authorname/Desktop/Projects/ml4h_project/LGBM(Genes+Demo)\"\n",
    "SEEDS     = [1,2,3,4,5]\n",
    "GENO_PARQUET      = \"/Users/authorname/Downloads/all_chr_merged.parquet\"\n",
    "GWAS_CATALOG_TSV  = \"/Users/authorname/Downloads/MONDO_0004975_associations_export.tsv\"\n",
    "GTEX_BRAIN_DIR = \"/Users/authorname/Downloads/GTEx_v10_SuSiE_eQTL\"\n",
    "GTEX_GLOB      = os.path.join(GTEX_BRAIN_DIR, \"Brain_*eQTLs.SuSiE_summary.parquet\")\n",
    "OUT_DIR = os.path.join(MODEL_DIR, \"eqtl_overlap_results\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "NON_GENE_FEATURES_EXACT = {\"age_death\",\"educ\",\"msex\"}\n",
    "NON_GENE_PREFIXES = (\"apoe_genotype_\",)\n",
    "PIP_MIN = None\n",
    "\n",
    "def safe_cls(c: str) -> str:\n",
    "    return c.replace(\"+\",\"plus\").replace(\" \",\"_\").replace(\"/\",\"-\")\n",
    "def unsafify_cls(s: str) -> str:\n",
    "    return s.replace(\"plus\",\"+\").replace(\"_\",\" \").replace(\"-\",\"/\")\n",
    "def get_feature_names(automl):\n",
    "    model = automl.model\n",
    "    est = getattr(model, \"estimator\", model)\n",
    "    if hasattr(est, \"feature_name_\") and est.feature_name_ is not None:\n",
    "        return list(est.feature_name_)\n",
    "    if hasattr(automl, \"feature_names_in_\") and automl.feature_names_in_ is not None:\n",
    "        return list(automl.feature_names_in_)\n",
    "    n_feats = getattr(est, \"n_features_\", None) or getattr(est, \"n_features_in_\", None)\n",
    "    if n_feats is None:\n",
    "        raise RuntimeError(\"Could not determine feature names.\")\n",
    "    return [f\"feat_{i}\" for i in range(n_feats)]\n",
    "def get_importances(automl):\n",
    "    model = automl.model\n",
    "    est = getattr(model, \"estimator\", model)\n",
    "    try:\n",
    "        booster = est.booster_\n",
    "        imp_gain = booster.feature_importance(importance_type=\"gain\")\n",
    "        if np.sum(imp_gain) > 0:\n",
    "            return imp_gain.astype(float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if hasattr(est, \"feature_importances_\"):\n",
    "        return est.feature_importances_.astype(float)\n",
    "    raise RuntimeError(\"No importances on fitted estimator.\")\n",
    "\n",
    "# %% Discover classes (unchanged)\n",
    "patt = re.compile(r\"seed(\\d+)_(.+)_automl\\.pkl$\")\n",
    "classes = sorted({unsafify_cls(m.group(2)) for fn in os.listdir(MODEL_DIR) if (m:=patt.match(fn))})\n",
    "print(f\"Discovered classes: {classes}\")\n",
    "\n",
    "# %% STEP A — Rebuild SNP→gene map + add COORDS  --------------------------------\n",
    "print(\"\\n[STEP A] Loading genotype + GWAS catalog, rebuilding SNP→gene map with QC ...\")\n",
    "df = pd.read_parquet(GENO_PARQUET)\n",
    "gwas_df = pd.read_csv(GWAS_CATALOG_TSV, sep=\"\\t\")\n",
    "\n",
    "# Clean GWAS fields (unchanged)\n",
    "gwas_df['riskAllele_cleaned'] = gwas_df['riskAllele'].str.extract(r'(rs\\d+)', expand=False)\n",
    "gwas_df['first_gene'] = gwas_df['mappedGenes'].str.split(',').str[0].str.strip()\n",
    "\n",
    "# >>> UPDATED/NEW <<<  pick a locations column and normalize to 'chr{chrom}_{pos}'\n",
    "loc_col = next((c for c in ['locations','location','chromosomeLocation','CHR_POS'] if c in gwas_df.columns), None)\n",
    "def norm_loc_to_chrpos(val):\n",
    "    if pd.isna(val): return None\n",
    "    s = str(val).split(';')[0].strip()\n",
    "    m = re.match(r'^(?:chr)?(\\w+):(\\d+)', s)  # matches '1:1049997' or 'chr1:1049997' or '1:1049997-...'\n",
    "    if m:\n",
    "        return f\"chr{m.group(1)}_{m.group(2)}\"\n",
    "    return None\n",
    "gwas_df['chr_pos'] = gwas_df[loc_col].apply(norm_loc_to_chrpos) if loc_col else None\n",
    "\n",
    "# rsID -> gene (as before)\n",
    "rs_to_gene = (gwas_df.dropna(subset=['riskAllele_cleaned','first_gene'])\n",
    "                     .set_index('riskAllele_cleaned')['first_gene'].to_dict())\n",
    "\n",
    "# >>> NEW <<< rsID -> chr_pos (to recover coords for rs-only columns)\n",
    "rs_to_chrpos = (gwas_df.dropna(subset=['riskAllele_cleaned','chr_pos'])\n",
    "                        .drop_duplicates(subset=['riskAllele_cleaned','chr_pos'])\n",
    "                        .set_index('riskAllele_cleaned')['chr_pos'].to_dict())\n",
    "\n",
    "# Identify SNP columns and keep those we can map via rsID\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "snp_cols_raw = [c for c in df.columns if c.startswith('rs') or c.startswith('chr')]\n",
    "\n",
    "def base_rsid(col):  # 'rs123_A' -> 'rs123'\n",
    "    return col.split('_')[0]\n",
    "\n",
    "# rsID-based map to genes (unchanged)\n",
    "all_mapped_snps = {col: rs_to_gene.get(base_rsid(col))\n",
    "                   for col in snp_cols_raw if rs_to_gene.get(base_rsid(col))}\n",
    "print(f\"Initially mapped {len(all_mapped_snps)} SNP columns to genes via GWAS catalog.\")\n",
    "\n",
    "# QC (unchanged)\n",
    "def calc_maf(col):\n",
    "    x = df[col]\n",
    "    return (2*x.eq(2).sum() + x.eq(1).sum()) / (2*x.notna().sum())\n",
    "snps_to_map = list(all_mapped_snps.keys())\n",
    "maf = pd.Series({col: calc_maf(col) for col in snps_to_map})\n",
    "miss = df[snps_to_map].isna().mean()\n",
    "snps_remove = set(maf[maf < 0.01].index.tolist()) | set(miss[miss > 0.05].index.tolist())\n",
    "all_mapped_snps_qc = {snp: gene for snp, gene in all_mapped_snps.items() if snp not in snps_remove}\n",
    "print(f\"Retained {len(all_mapped_snps_qc)} SNPs after QC.\")\n",
    "\n",
    "# Build gene -> list of SNP column names (unchanged)\n",
    "from collections import defaultdict\n",
    "gene_to_snpcols = defaultdict(list)\n",
    "for snp_col, gene in all_mapped_snps_qc.items():\n",
    "    gene_to_snpcols[gene].append(snp_col)\n",
    "\n",
    "# >>> NEW <<<  derive a normalized chr_pos for each SNP column\n",
    "chrpos_regex = re.compile(r'chr(\\w+)[_:](\\d+)')  # matches 'chr1:285155' or 'chr1_285155'\n",
    "def col_to_chrpos(col):\n",
    "    # 1) try to parse coordinate from the column name directly\n",
    "    m = chrpos_regex.search(col)\n",
    "    if m:\n",
    "        return f\"chr{m.group(1)}_{m.group(2)}\"\n",
    "    # 2) if it's an rsID column, try the GWAS rsID→chr_pos map\n",
    "    rs = base_rsid(col) if col.startswith('rs') else None\n",
    "    if rs and rs in rs_to_chrpos:\n",
    "        return rs_to_chrpos[rs]\n",
    "    return None\n",
    "\n",
    "# gene -> set of chr_pos used by your model for that gene\n",
    "gene_to_chrpos_used = {g: {cp for cp in (col_to_chrpos(c) for c in cols) if cp is not None}\n",
    "                       for g, cols in gene_to_snpcols.items()}\n",
    "\n",
    "# %% STEP B — Load GTEx and add chr_pos from variant_id  ------------------------\n",
    "print(\"\\n[STEP B] Loading GTEx v10 SuSiE brain cis-eQTL parquet files ...\")\n",
    "gtex_files = sorted(glob.glob(GTEX_GLOB))\n",
    "if not gtex_files:\n",
    "    raise RuntimeError(\"No GTEx brain parquet files found. Check GTEX_GLOB path/pattern.\")\n",
    "\n",
    "eqtl_rows = []\n",
    "for path in gtex_files:\n",
    "    tissue = os.path.basename(path).split(\".v10.eQTLs.SuSiE_summary.parquet\")[0]\n",
    "    tdf = pd.read_parquet(path)\n",
    "    if \"gene_name\" not in tdf.columns or \"variant_id\" not in tdf.columns:\n",
    "        continue\n",
    "    # >>> UPDATED/NEW <<< make 'chr_pos' = first two tokens of variant_id: 'chrX_pos'\n",
    "    # variant_id looks like 'chr1_285155_A_C_b38' (indels possible in ref/alt, this still works)\n",
    "    parts = tdf[\"variant_id\"].str.split(\"_\", n=4, expand=True)\n",
    "    tdf[\"chr_pos\"] = parts[0] + \"_\" + parts[1]  # 'chr1' + '_' + '285155'\n",
    "    # optional PIP filter\n",
    "    if PIP_MIN is not None and \"pip\" in tdf.columns:\n",
    "        tdf = tdf.loc[tdf[\"pip\"] >= PIP_MIN]\n",
    "    tdf = tdf[[\"gene_name\",\"chr_pos\"] + ([\"pip\"] if \"pip\" in tdf.columns else [])].copy()\n",
    "    tdf[\"tissue\"] = tissue\n",
    "    eqtl_rows.append(tdf)\n",
    "\n",
    "if not eqtl_rows:\n",
    "    raise RuntimeError(\"No usable GTEx rows parsed.\")\n",
    "gtex_eqtl = pd.concat(eqtl_rows, ignore_index=True)\n",
    "\n",
    "# Lookups: gene -> set(chr_pos)\n",
    "gene_to_gtex_chrpos = (\n",
    "    gtex_eqtl.groupby(\"gene_name\")[\"chr_pos\"].apply(set).to_dict()\n",
    ")\n",
    "\n",
    "# Details per (gene, chr_pos): tissues and max PIP (if present)\n",
    "if \"pip\" in gtex_eqtl.columns:\n",
    "    eqtl_detail = (gtex_eqtl.groupby([\"gene_name\",\"chr_pos\"])\n",
    "                           .agg(tissues=(\"tissue\", lambda x: sorted(set(x))),\n",
    "                                max_pip=(\"pip\",\"max\")).reset_index())\n",
    "else:\n",
    "    eqtl_detail = (gtex_eqtl.groupby([\"gene_name\",\"chr_pos\"])\n",
    "                           .agg(tissues=(\"tissue\", lambda x: sorted(set(x)))).reset_index())\n",
    "    eqtl_detail[\"max_pip\"] = np.nan\n",
    "\n",
    "print(f\"GTEx eQTL rows loaded: {len(gtex_eqtl):,}\")\n",
    "\n",
    "# %% STEP C — Predictive genes per class (unchanged)\n",
    "print(\"\\n[STEP C] Identifying predictive genes per class (non-zero importance in ≥2 seeds) ...\")\n",
    "\n",
    "def is_non_gene(feature_name: str) -> bool:\n",
    "    if feature_name in NON_GENE_FEATURES_EXACT:\n",
    "        return True\n",
    "    return any(feature_name.startswith(p) for p in NON_GENE_PREFIXES)\n",
    "\n",
    "all_gene_names = set(gene_to_snpcols.keys())\n",
    "predictive_genes_per_class, counts_per_class = {}, {}\n",
    "\n",
    "for cls in classes:\n",
    "    counts = {}\n",
    "    for seed in SEEDS:\n",
    "        pkl_path = os.path.join(MODEL_DIR, f\"seed{seed}_{safe_cls(cls)}_automl.pkl\")\n",
    "        if not os.path.exists(pkl_path):\n",
    "            print(f\"[skip] Missing {pkl_path}\")\n",
    "            continue\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            automl = pickle.load(f)\n",
    "        feats = get_feature_names(automl)\n",
    "        imps  = get_importances(automl)\n",
    "        if len(feats) != len(imps):\n",
    "            raise ValueError(f\"Mismatch in feature name/importance lengths for {cls}, seed {seed}\")\n",
    "        nonzero = [f for f, imp in zip(feats, imps) if imp > 0]\n",
    "        nonzero_genes = [f for f in nonzero if (f in all_gene_names and not is_non_gene(f))]\n",
    "        for g in nonzero_genes:\n",
    "            counts[g] = counts.get(g, 0) + 1\n",
    "    counts_per_class[cls] = counts\n",
    "    predictive_genes_per_class[cls] = sorted([g for g, c in counts.items() if c >= 2])\n",
    "    print(f\"[{cls}] predictive genes (≥2 seeds): {len(predictive_genes_per_class[cls])}\")\n",
    "\n",
    "pd.DataFrame.from_dict(counts_per_class, orient=\"index\").T.to_csv(\n",
    "    os.path.join(OUT_DIR, \"predictive_gene_counts_per_class.csv\")\n",
    ")\n",
    "\n",
    "# %% STEP D — COORDINATE-LEVEL overlap with GTEx (per class & gene) ------------\n",
    "print(\"\\n[STEP D] Computing coordinate-level overlaps with GTEx brain cis-eQTLs ...\")\n",
    "\n",
    "rows = []\n",
    "for cls in classes:\n",
    "    for g in predictive_genes_per_class.get(cls, []):\n",
    "        used_chrpos = gene_to_chrpos_used.get(g, set())\n",
    "        gtex_chrpos = gene_to_gtex_chrpos.get(g, set())\n",
    "        overlap = sorted(used_chrpos & gtex_chrpos)\n",
    "        # details\n",
    "        detail_list = []\n",
    "        if overlap:\n",
    "            det = eqtl_detail[(eqtl_detail[\"gene_name\"] == g) & (eqtl_detail[\"chr_pos\"].isin(overlap))]\n",
    "            for _, r in det.iterrows():\n",
    "                detail_list.append({\n",
    "                    \"chr_pos\": r[\"chr_pos\"],\n",
    "                    \"tissues\": \";\".join(r[\"tissues\"]),\n",
    "                    \"max_pip\": r.get(\"max_pip\", np.nan)\n",
    "                })\n",
    "        rows.append({\n",
    "            \"class\": cls,\n",
    "            \"gene\": g,\n",
    "            \"n_chrpos_used\": len(used_chrpos),\n",
    "            \"n_overlap_chrpos\": len(overlap),\n",
    "            \"overlap_chrpos\": \";\".join(overlap),\n",
    "            \"overlap_detail\": detail_list,\n",
    "            \"any_eqtl_anyBrain\": (g in gene_to_gtex_chrpos)\n",
    "        })\n",
    "\n",
    "overlap_df = pd.DataFrame(rows)\n",
    "overlap_path = os.path.join(OUT_DIR, \"eqtl_coord_overlap_per_gene_per_class.csv\")\n",
    "if not overlap_df.empty:\n",
    "    overlap_df[\"overlap_detail_json\"] = overlap_df[\"overlap_detail\"].apply(lambda x: \"\" if not x else str(x))\n",
    "    overlap_df.drop(columns=[\"overlap_detail\"], inplace=True)\n",
    "overlap_df.to_csv(overlap_path, index=False)\n",
    "print(f\"[saved] {overlap_path}\")\n",
    "\n",
    "# Exploded details table\n",
    "detail_rows = []\n",
    "for _, r in overlap_df.iterrows():\n",
    "    if not r[\"overlap_chrpos\"]:\n",
    "        continue\n",
    "    cls, gene = r[\"class\"], r[\"gene\"]\n",
    "    for cp in r[\"overlap_chrpos\"].split(\";\"):\n",
    "        det = eqtl_detail[(eqtl_detail[\"gene_name\"] == gene) & (eqtl_detail[\"chr_pos\"] == cp)]\n",
    "        for _, d in det.iterrows():\n",
    "            detail_rows.append({\n",
    "                \"class\": cls,\n",
    "                \"gene\": gene,\n",
    "                \"chr_pos\": cp,\n",
    "                \"tissues\": \";\".join(d[\"tissues\"]),\n",
    "                \"max_pip\": d.get(\"max_pip\", np.nan)\n",
    "            })\n",
    "detail_df = pd.DataFrame(detail_rows)\n",
    "if not detail_df.empty:\n",
    "    detail_path = os.path.join(OUT_DIR, \"eqtl_coord_overlap_details.csv\")\n",
    "    detail_df.to_csv(detail_path, index=False)\n",
    "    print(f\"[saved] {detail_path}\")\n",
    "\n",
    "# %% Summary\n",
    "print(\"\\n=== Summary (coordinate-based) ===\")\n",
    "print(overlap_df.groupby(\"class\")[\"n_overlap_chrpos\"].sum().rename(\"total_overlapping_chrpos_by_class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "overlap_path = \"/Users/authorname/Desktop/Projects/ml4h_project/LGBM(Genes+Demo)/eqtl_overlap_results/eqtl_coord_overlap_per_gene_per_class.csv\"\n",
    "df = pd.read_csv(overlap_path)\n",
    "\n",
    "# filter to only overlapping genes\n",
    "hits = df[df[\"n_overlap_chrpos\"] > 0]\n",
    "\n",
    "print(\"=== Overlapping genes by class ===\")\n",
    "for cls, sub in hits.groupby(\"class\"):\n",
    "    genes = sub[\"gene\"].tolist()\n",
    "    print(f\"{cls} ({len(genes)}): {', '.join(genes)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neuro_240)",
   "language": "python",
   "name": "neuro_240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
