{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, matthews_corrcoef,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# ---- Folders and model types to evaluate ----\n",
    "model_folders = {\n",
    "    \"RF (Proteomics + Demographics)\": \"/Users/authorname/Desktop/Projects/proteomics_RF(ANML+Meta)\",\n",
    "    \"LGBM (Proteomics + Demographics)\": \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\",\n",
    "    \"LGBM (Protein Ratios)\": \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)_RATIOS_fixed_rfe\",\n",
    "    \"LGBM (Protein Ratios + Demographics)\": \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)_RATIOS_Demo\",\n",
    "    \"RF (Genes + Demographics)\": \"/Users/authorname/Desktop/Projects/ml4h_project/RF(Genes+Demo)\",\n",
    "    \"LGBM (Genes + Demographics)\": \"/Users/authorname/Desktop/Projects/ml4h_project/LGBM(Genes+Demo)\",\n",
    "    \"LGBM (Gene Ratios)\": \"/Users/authorname/Desktop/Projects/Genomics_LGBM(Genomics+Meta)_RATIOS_fixed_rfe\",\n",
    "    \"LGBM (Gene Ratios + Demographics)\": \"/Users/authorname/Desktop/Projects/Genomics_LGBM(Genomics+Meta)_RATIOS_Demo\"\n",
    "}\n",
    "\n",
    "\n",
    "# ---- Metrics to compute ----\n",
    "def compute_metrics(y_true, y_score, y_pred):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"ROC AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"PR AUC\": average_precision_score(y_true, y_score),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# ---- Find Youden's J optimal threshold ----\n",
    "def optimal_threshold_youden(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    youden_j = tpr - fpr\n",
    "    best_idx = np.argmax(youden_j)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "# ---- Aggregate per-model results ----\n",
    "rows = []\n",
    "\n",
    "for model_name, folder in model_folders.items():\n",
    "    files = glob.glob(os.path.join(folder, \"seed*_*.csv\"))\n",
    "    if not files:\n",
    "        print(f\"No files found for {model_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Extract class names\n",
    "    def extract_class(p): return \"_\".join(os.path.basename(p).split(\"_\")[1:]).replace(\".csv\", \"\")\n",
    "    class_names = sorted(set(extract_class(f) for f in files))\n",
    "\n",
    "    for cls in class_names:\n",
    "        cls_files = sorted(glob.glob(os.path.join(folder, f\"seed*_{cls}.csv\")))\n",
    "\n",
    "        metric_lists = {k: [] for k in [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\", \"PR AUC\", \"MCC\"]}\n",
    "\n",
    "        for f in cls_files:\n",
    "            df = pd.read_csv(f)\n",
    "            y_true = df[\"y_true\"].astype(int).values\n",
    "            y_score = df[\"y_score\"].astype(float).values\n",
    "\n",
    "            try:\n",
    "                # Compute optimal threshold for Youden's J\n",
    "                opt_thresh = optimal_threshold_youden(y_true, y_score)\n",
    "                y_pred = (y_score >= opt_thresh).astype(int)\n",
    "\n",
    "                metrics = compute_metrics(y_true, y_score, y_pred)\n",
    "                for k, v in metrics.items():\n",
    "                    metric_lists[k].append(v)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {f}: {e}\")\n",
    "\n",
    "        # Aggregate mean ± std\n",
    "        summary = {\n",
    "            \"Model\": model_name,\n",
    "            \"Class\": cls\n",
    "        }\n",
    "        for k in metric_lists:\n",
    "            vals = np.array(metric_lists[k])\n",
    "            summary[f\"{k} (mean±std)\"] = f\"{np.mean(vals):.3f} ± {np.std(vals):.3f}\"\n",
    "        rows.append(summary)\n",
    "\n",
    "# ---- Final DataFrame ----\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df = summary_df.sort_values(by=[\"Model\", \"Class\"])\n",
    "summary_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ---- Save ----\n",
    "summary_df.to_csv(\"ML4H_model_eval_summary.csv\", index=False)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---- Load summary CSV ----\n",
    "df = pd.read_csv(\"ML4H_model_eval_summary.csv\")\n",
    "\n",
    "# ---- Exact model-name abbreviations (aligned with your model_folders keys) ----\n",
    "model_map = {\n",
    "    \"RF (Proteomics + Demographics)\":              \"RF (P + D)\",\n",
    "    \"LGBM (Proteomics + Demographics)\":            \"LGBM (P + D)\",\n",
    "    \"LGBM (Protein Ratios)\":                        \"LGBM (PR)\",\n",
    "    \"LGBM (Protein Ratios + Demographics)\":         \"LGBM (PR + D)\",\n",
    "    \"RF (Genes + Demographics)\":                    \"RF (G + D)\",\n",
    "    \"LGBM (Genes + Demographics)\":                  \"LGBM (G + D)\",\n",
    "    \"LGBM (Gene Ratios)\":                           \"LGBM (GR)\",\n",
    "    \"LGBM (Gene Ratios + Demographics)\":            \"LGBM (GR + D)\",\n",
    "}\n",
    "\n",
    "# ---- Class abbreviations (as previously specified) ----\n",
    "class_map = {\n",
    "    \"Typical_AD\":         \"AD\",\n",
    "    \"Healthy_Control\":    \"Control\",\n",
    "    \"Symptomatic_Non-AD\": \"Symp non-AD\",\n",
    "    \"Resilient\":          \"Resilient\",\n",
    "}\n",
    "\n",
    "# ---- Apply mappings with checks ----\n",
    "orig_models = df[\"Model\"].unique().tolist()\n",
    "orig_classes = df[\"Class\"].unique().tolist()\n",
    "\n",
    "df[\"Model_abbr\"] = df[\"Model\"].map(model_map)\n",
    "df[\"Class_abbr\"] = df[\"Class\"].replace(class_map)\n",
    "\n",
    "# Report any unmapped entries (None/NaN after mapping)\n",
    "unmapped_models = sorted(set(df.loc[df[\"Model_abbr\"].isna(), \"Model\"]))\n",
    "unmapped_classes = sorted(set(df.loc[df[\"Class_abbr\"].isna(), \"Class\"]))\n",
    "\n",
    "if unmapped_models:\n",
    "    print(\"[WARN] Unmapped models (update model_map):\", unmapped_models)\n",
    "if unmapped_classes:\n",
    "    print(\"[WARN] Unmapped classes (update class_map):\", unmapped_classes)\n",
    "\n",
    "# Use original value if not mapped (safety fallback)\n",
    "df[\"Model_abbr\"] = df[\"Model_abbr\"].fillna(df[\"Model\"])\n",
    "df[\"Class_abbr\"] = df[\"Class_abbr\"].fillna(df[\"Class\"])\n",
    "\n",
    "# ---- Keep only the requested columns/metrics ----\n",
    "cols_out = [\n",
    "    \"Model_abbr\",\n",
    "    \"Class_abbr\",\n",
    "    \"Accuracy (mean±std)\",\n",
    "    \"Precision (mean±std)\",\n",
    "    \"Recall (mean±std)\",\n",
    "    \"F1 (mean±std)\",\n",
    "    \"ROC AUC (mean±std)\",\n",
    "]\n",
    "\n",
    "df = df[cols_out].copy()\n",
    "df.rename(columns={\n",
    "    \"Model_abbr\": \"Model\",\n",
    "    \"Class_abbr\": \"Class\",\n",
    "    \"Accuracy (mean±std)\": \"Accuracy\",\n",
    "    \"Precision (mean±std)\": \"Precision\",\n",
    "    \"Recall (mean±std)\": \"Recall\",\n",
    "    \"F1 (mean±std)\": \"F1\",\n",
    "    \"ROC AUC (mean±std)\": \"AUROC\",\n",
    "}, inplace=True)\n",
    "\n",
    "# ---- Escape underscores for LaTeX ----\n",
    "df = df.replace(\"_\", r\"\\_\", regex=True)\n",
    "\n",
    "# ---- Build LaTeX table ----\n",
    "\n",
    "# ---- Build LaTeX table ----\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    row_str = \" & \".join(str(row[c]) for c in [\"Model\",\"Class\",\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"AUROC\"]) + r\" \\\\\"\n",
    "    rows.append(row_str)\n",
    "\n",
    "header_titles = [\"Model\", \"Class\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"]\n",
    "col_spec = \"l\" * len(header_titles)\n",
    "\n",
    "latex = r\"\"\"\n",
    "\\begin{table*}[htbp]\n",
    "\\floatconts\n",
    "  {tab:eval_core}%\n",
    "  {\\caption{Per-model, per-class evaluation (mean $\\pm$ std across 5 seeds).}}%\n",
    "  {%\n",
    "  \\scriptsize\n",
    "    \\begin{tabular}{\"\"\" + col_spec + r\"\"\"}\n",
    "    \\toprule\n",
    "    \"\"\" + \" & \".join(header_titles) + r\"\"\" \\\\\n",
    "    \\midrule\n",
    "\"\"\" + \"\\n\".join(rows) + r\"\"\"\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\par\n",
    "  }\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"ML4H_model_eval_summary_table_core.tex\", \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# --- Folders ---\n",
    "baseline_folder = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\"                 # LGBM (Proteomics)\n",
    "ratios_folder   = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)_RATIOS_fixed_rfe\"  # LGBM (Protein Ratios)\n",
    "\n",
    "classes = [\"MCI\", \"NCI\", \"AD\", \"AD+\"]\n",
    "seeds   = [1, 2, 3, 4, 5]\n",
    "\n",
    "def safe_cls(c):\n",
    "    return c.replace(\"+\",\"plus\").replace(\" \",\"_\").replace(\"/\",\"-\")\n",
    "\n",
    "def mean_auc_like_plot(folder):\n",
    "    \"\"\"Compute per-class mean AUC exactly like the plotting code:\n",
    "       for each seed file: fpr,tpr = roc_curve(...); mean auc = mean(auc(fpr,tpr)).\"\"\"\n",
    "    mean_by_class = {}\n",
    "    used_seeds = {}\n",
    "    for cls in classes:\n",
    "        aucs, used = [], []\n",
    "        for seed in seeds:\n",
    "            path = os.path.join(folder, f\"seed{seed}_{safe_cls(cls)}.csv\")\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            df = pd.read_csv(path)\n",
    "            y_true  = df[\"y_true\"].astype(int).values\n",
    "            y_score = df[\"y_score\"].astype(float).values\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)  # EXACTLY as in your plot\n",
    "            aucs.append(auc(fpr, tpr))                # no FPR dedup, no roc_auc_score\n",
    "            used.append(seed)\n",
    "        if aucs:\n",
    "            mean_by_class[cls] = float(np.mean(aucs))\n",
    "            used_seeds[cls] = used\n",
    "    return mean_by_class, used_seeds\n",
    "\n",
    "# Compute means for each folder\n",
    "base_mean, base_used = mean_auc_like_plot(baseline_folder)\n",
    "ratio_mean, ratio_used = mean_auc_like_plot(ratios_folder)\n",
    "\n",
    "# Build table and differences (unpaired means)\n",
    "common_classes = sorted(set(base_mean) & set(ratio_mean))\n",
    "rows = []\n",
    "for cls in common_classes:\n",
    "    rows.append({\n",
    "        \"Class\": cls,\n",
    "        \"base_mean_auc\": base_mean[cls],\n",
    "        \"ratio_mean_auc\": ratio_mean[cls],\n",
    "        \"diff\": ratio_mean[cls] - base_mean[cls],\n",
    "        \"base_seeds\": base_used.get(cls, []),\n",
    "        \"ratio_seeds\": ratio_used.get(cls, []),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"Class\")\n",
    "\n",
    "# Pretty print (rounding only for display)\n",
    "fmt = {\"base_mean_auc\": lambda x: f\"{x:.3f}\",\n",
    "       \"ratio_mean_auc\": lambda x: f\"{x:.3f}\",\n",
    "       \"diff\": lambda x: f\"{x:.3f}\"}\n",
    "print(df[[\"Class\",\"base_mean_auc\",\"ratio_mean_auc\",\"diff\"]].to_string(index=False, formatters=fmt))\n",
    "\n",
    "overall = df[\"diff\"].mean()\n",
    "print(f\"\\nAverage ROC AUC difference across classes: {overall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# --- Folders (Genomics) ---\n",
    "baseline_folder = \"/Users/authorname/Desktop/Projects/ml4h_project/LGBM(Genes+Demo)\"                 # LGBM (Genomics + Demographics)\n",
    "ratios_folder   = \"/Users/authorname/Desktop/Projects/Genomics_LGBM(Genomics+Meta)_RATIOS_Demo\"  # LGBM (Gene Ratios, no demo)\n",
    "\n",
    "# --- Classes (Genomics labels) ---\n",
    "classes = [\"Healthy Control\", \"Resilient\", \"Symptomatic Non-AD\", \"Typical AD\"]\n",
    "seeds   = [1, 2, 3, 4, 5]\n",
    "\n",
    "def safe_cls(c):\n",
    "    return c.replace(\"+\",\"plus\").replace(\" \",\"_\").replace(\"/\",\"-\")\n",
    "\n",
    "def mean_auc_like_plot(folder):\n",
    "    \"\"\"Compute per-class mean AUC exactly like the plotting code:\n",
    "       for each seed file: fpr,tpr = roc_curve(...); mean auc = mean(auc(fpr,tpr)).\"\"\"\n",
    "    mean_by_class, used_seeds = {}, {}\n",
    "    for cls in classes:\n",
    "        aucs, used = [], []\n",
    "        for seed in seeds:\n",
    "            path = os.path.join(folder, f\"seed{seed}_{safe_cls(cls)}.csv\")\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            df = pd.read_csv(path)\n",
    "            y_true  = df[\"y_true\"].astype(int).values\n",
    "            y_score = df[\"y_score\"].astype(float).values\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)  # EXACTLY as in your plot\n",
    "            aucs.append(auc(fpr, tpr))                # no FPR dedup, no roc_auc_score\n",
    "            used.append(seed)\n",
    "        if aucs:\n",
    "            mean_by_class[cls] = float(np.mean(aucs))\n",
    "            used_seeds[cls] = used\n",
    "    return mean_by_class, used_seeds\n",
    "\n",
    "# Compute means for each folder\n",
    "base_mean, base_used = mean_auc_like_plot(baseline_folder)  # Genomics + Demo\n",
    "ratio_mean, ratio_used = mean_auc_like_plot(ratios_folder)  # Gene Ratios (no demo)\n",
    "\n",
    "# Build table and differences (unpaired means)\n",
    "common_classes = sorted(set(base_mean) & set(ratio_mean))\n",
    "rows = []\n",
    "for cls in common_classes:\n",
    "    rows.append({\n",
    "        \"Class\": cls,\n",
    "        \"base_mean_auc\": base_mean[cls],\n",
    "        \"ratio_mean_auc\": ratio_mean[cls],\n",
    "        \"diff\": ratio_mean[cls] - base_mean[cls],\n",
    "        \"base_seeds\": base_used.get(cls, []),\n",
    "        \"ratio_seeds\": ratio_used.get(cls, []),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"Class\")\n",
    "\n",
    "# Pretty print (rounding only for display)\n",
    "fmt = {\"base_mean_auc\": lambda x: f\"{x:.3f}\",\n",
    "       \"ratio_mean_auc\": lambda x: f\"{x:.3f}\",\n",
    "       \"diff\": lambda x: f\"{x:.3f}\"}\n",
    "print(df[[\"Class\",\"base_mean_auc\",\"ratio_mean_auc\",\"diff\"]].to_string(index=False, formatters=fmt))\n",
    "\n",
    "overall = df[\"diff\"].mean() if not df.empty else float(\"nan\")\n",
    "print(f\"\\nAverage ROC AUC difference across classes: {overall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neuro_240)",
   "language": "python",
   "name": "neuro_240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
