{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base = \"/Users/authorname/Downloads/syn65414912\"\n",
    "\n",
    "# Load the key files\n",
    "df_levels_anml   = pd.read_csv(os.path.join(base, \"OhNM2025_ROSMAP_plasma_Soma7k_protein_level_ANML_log10.csv\"))\n",
    "df_levels_log10  = pd.read_csv(os.path.join(base, \"OhNM2025_ROSMAP_plasma_Soma7k_protein_level_log10.csv\"))\n",
    "df_protein_meta  = pd.read_csv(os.path.join(base, \"OhNM2025_ROSMAP_plasma_Soma7k_protein_metadata.csv\"))\n",
    "df_sample_meta   = pd.read_csv(os.path.join(base, \"OhNM2025_ROSMAP_plasma_Soma7k_sample_metadata.csv\"))\n",
    "\n",
    "# Quick checks\n",
    "print(\"Protein levels (ANML):\", df_levels_anml.shape)\n",
    "print(\"Protein levels (log10):\", df_levels_log10.shape)\n",
    "print(\"Protein metadata:\", df_protein_meta.shape)\n",
    "print(\"Sample metadata:\", df_sample_meta.shape)\n",
    "\n",
    "# Preview sample metadata (patients)\n",
    "print(\"\\n=== Sample Metadata Head ===\")\n",
    "print(df_sample_meta.head())\n",
    "\n",
    "# Preview protein metadata\n",
    "print(\"\\n=== Protein Metadata Head ===\")\n",
    "print(df_protein_meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 1 — PREPROCESSING\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ---- File paths ----\n",
    "BASE = \"/Users/authorname/Downloads/syn65414912\"\n",
    "ANML_PATH   = os.path.join(BASE, \"OhNM2025_ROSMAP_plasma_Soma7k_protein_level_ANML_log10.csv\")\n",
    "SAMPLE_PATH = os.path.join(BASE, \"OhNM2025_ROSMAP_plasma_Soma7k_sample_metadata.csv\")\n",
    "\n",
    "# ---- Load ----\n",
    "df_levels = pd.read_csv(ANML_PATH)        # rows: projid_visit, cols: proteins\n",
    "df_meta   = pd.read_csv(SAMPLE_PATH)      # contains projid_visit, projid, msex, age_at_visit, educ, apoe_genotype, Diagnosis\n",
    "\n",
    "# ---- Sanity: keys present? ----\n",
    "assert \"projid_visit\" in df_levels.columns, \"projid_visit missing in protein matrix.\"\n",
    "for col in [\"projid_visit\",\"projid\",\"msex\",\"age_at_visit\",\"educ\",\"apoe_genotype\",\"Diagnosis\"]:\n",
    "    assert col in df_meta.columns, f\"{col} missing in sample metadata.\"\n",
    "\n",
    "# ---- Align on visit ----\n",
    "# inner join so we only keep visits present in BOTH\n",
    "df = pd.merge(df_meta, df_levels, on=\"projid_visit\", how=\"inner\", validate=\"one_to_one\")\n",
    "print(\"Aligned shape:\", df.shape)\n",
    "\n",
    "# ---- Labels (four groups) ----\n",
    "df[\"Diagnosis\"] = df[\"Diagnosis\"].astype(str).str.strip()\n",
    "valid_classes = {\"MCI\",\"NCI\",\"AD\",\"AD+\"}\n",
    "df = df[df[\"Diagnosis\"].isin(valid_classes)].reset_index(drop=True)\n",
    "print(\"Class counts:\\n\", df[\"Diagnosis\"].value_counts())\n",
    "\n",
    "# ---- Grouping key for leakage control ----\n",
    "df[\"projid\"] = df[\"projid\"].astype(str)\n",
    "\n",
    "# ---- Stratification label: Diagnosis × sex (0/1) ----\n",
    "df[\"msex\"] = df[\"msex\"].astype(int)\n",
    "df[\"strata\"] = df[\"Diagnosis\"] + \"_\" + df[\"msex\"].astype(str)\n",
    "\n",
    "# ---- APOE one-hot (Unknown for NaN) ----\n",
    "def format_apoe(x):\n",
    "    if pd.isna(x):\n",
    "        return \"Unknown\"\n",
    "    try:\n",
    "        # e.g., 33.0 -> \"33\"\n",
    "        return str(int(float(x)))\n",
    "    except Exception:\n",
    "        s = str(x).strip()\n",
    "        return s if s else \"Unknown\"\n",
    "\n",
    "df[\"apoe_str\"] = df[\"apoe_genotype\"].apply(format_apoe)\n",
    "\n",
    "# scikit-learn compatibility across versions\n",
    "try:\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "apoe_ohe = ohe.fit_transform(df[[\"apoe_str\"]])\n",
    "apoe_cols = [c.replace(\"apoe_str_\",\"APOE_\") for c in ohe.get_feature_names_out()]\n",
    "df_apoe  = pd.DataFrame(apoe_ohe, columns=apoe_cols, index=df.index)\n",
    "\n",
    "# ---- Numeric covariates (unscaled) ----\n",
    "for col in [\"age_at_visit\",\"educ\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ---- Protein feature columns ----\n",
    "protein_cols = [c for c in df_levels.columns if c != \"projid_visit\"]\n",
    "\n",
    "# ---- Final feature matrix X ----\n",
    "X = pd.concat([df[[\"age_at_visit\",\"educ\"]], df_apoe, df[protein_cols]], axis=1)\n",
    "y = df[\"Diagnosis\"].astype(str).values\n",
    "groups = df[\"projid\"].values\n",
    "strata = df[\"strata\"].values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"APOE levels seen:\", sorted(set(df[\"apoe_str\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISIT-LEVEL SUMMARY (standalone) ===\n",
    "print(\"\\n=== Visit-level summary ===\")\n",
    "print(f\"Total patient visits: {len(df)}\")\n",
    "\n",
    "# Visits per diagnosis class (NCI, MCI, AD, AD+), missing classes shown as 0\n",
    "order = [\"NCI\", \"MCI\", \"AD\", \"AD+\"]\n",
    "counts = df[\"Diagnosis\"].value_counts().reindex(order).fillna(0).astype(int)\n",
    "\n",
    "print(\"Visits per class:\")\n",
    "for cls, cnt in counts.items():\n",
    "    print(f\"  {cls}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — TRAINING RF\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---- Output folder ----\n",
    "out_dir = \"/Users/authorname/Desktop/Projects/proteomics_RF(ANML+Meta)\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---- Helper: group-aware stratified 70/30 split (by mode of strata per subject) ----\n",
    "def group_stratified_shuffle_split(df_index, strata_all, groups_all, test_size=0.30, random_state=0):\n",
    "    \"\"\"Return train_idx, test_idx ensuring groups stay intact and class-sex balance is approx preserved.\n",
    "       Strategy: assign each group a single stratum = mode(strata) among its rows, then stratified split at group level.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    data = pd.DataFrame({\"idx\": df_index, \"strata\": strata_all, \"group\": groups_all})\n",
    "\n",
    "    # group -> mode(strata)\n",
    "    grp_mode = (\n",
    "        data.groupby(\"group\")[\"strata\"]\n",
    "        .agg(lambda s: s.value_counts().idxmax())\n",
    "    )\n",
    "    grp_mode = grp_mode.sample(frac=1.0, random_state=random_state)  # shuffle groups\n",
    "\n",
    "    train_groups, test_groups = [], []\n",
    "    for s_val, grp_ids in grp_mode.groupby(grp_mode.values):\n",
    "        g_list = list(grp_ids.index)\n",
    "        rng.shuffle(g_list)\n",
    "        n_test = max(1, int(round(test_size * len(g_list))))\n",
    "        test_groups.extend(g_list[:n_test])\n",
    "        train_groups.extend(g_list[n_test:])\n",
    "\n",
    "    train_mask = np.isin(groups_all, train_groups)\n",
    "    test_mask  = np.isin(groups_all,  test_groups)\n",
    "    return np.where(train_mask)[0], np.where(test_mask)[0]\n",
    "\n",
    "# ---- Classes & seeds ----\n",
    "classes = [\"MCI\",\"NCI\",\"AD\",\"AD+\"]     # will save AD+ as 'ADplus' in filenames\n",
    "def safe_cls(c): return c.replace(\"+\",\"plus\").replace(\" \",\"_\").replace(\"/\",\"-\")\n",
    "\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "# ---- Main loop ----\n",
    "for seed in seeds:\n",
    "    tr_idx, te_idx = group_stratified_shuffle_split(\n",
    "        df_index=np.arange(len(X)),\n",
    "        strata_all=strata,\n",
    "        groups_all=groups,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_train_full    = y[tr_idx]\n",
    "    y_test_full     = y[te_idx]\n",
    "\n",
    "    print(f\"\\n[Seed {seed}] Train n={len(tr_idx)} | Test n={len(te_idx)} | \"\n",
    "          f\"Groups train={len(np.unique(groups[tr_idx]))} test={len(np.unique(groups[te_idx]))}\")\n",
    "\n",
    "    for cls in classes:\n",
    "        y_train = (y_train_full == cls).astype(int)\n",
    "        y_test  = (y_test_full  == cls).astype(int)\n",
    "\n",
    "        automl = AutoML()\n",
    "        settings = {\n",
    "            \"time_budget\": 100,               # seconds per class; adjust if you want longer searches\n",
    "            \"metric\": \"roc_auc\",\n",
    "            \"task\": \"classification\",\n",
    "            \"eval_method\": \"cv\",\n",
    "            \"estimator_list\": [\"rf\"],       # force random forest\n",
    "            \"log_file_name\": os.path.join(out_dir, f\"flaml_seed{seed}_{safe_cls(cls)}.log\"),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "        automl.pickle(os.path.join(out_dir, f\"seed{seed}_{safe_cls(cls)}_automl.pkl\"))\n",
    "\n",
    "        # Predict proba for positive class\n",
    "        y_score = automl.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score) if (y_test.sum() > 0 and y_test.sum() < len(y_test)) else float(\"nan\")\n",
    "        print(f\"  [{cls}] AUC={auc:.3f}\")\n",
    "\n",
    "        # Save per-class CSV for your existing averaging/plotting script\n",
    "        out_csv = os.path.join(out_dir, f\"seed{seed}_{safe_cls(cls)}.csv\")\n",
    "        pd.DataFrame({\"y_true\": y_test.astype(int), \"y_score\": y_score.astype(float)}).to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ---- Mean ROC across seeds from saved CSVs ----\n",
    "folder = \"/Users/authorname/Desktop/Projects/proteomics_RF(ANML+Meta)\"\n",
    "files = glob.glob(os.path.join(folder, \"seed*_*.csv\"))\n",
    "assert len(files) > 0, \"No per-seed ROC CSVs found.\"\n",
    "\n",
    "# Infer class names from filenames (everything after 'seedX_')\n",
    "def class_from_path(p):\n",
    "    base = os.path.basename(p)\n",
    "    return \"_\".join(base.split(\"_\")[1:]).rsplit(\".csv\", 1)[0]\n",
    "\n",
    "class_names = sorted(set(class_from_path(p) for p in files))\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cls in class_names:\n",
    "    cls_files = sorted(glob.glob(os.path.join(folder, f\"seed*_{cls}.csv\")))\n",
    "    if not cls_files:\n",
    "        continue\n",
    "\n",
    "    tprs_interp, aucs = [], []\n",
    "\n",
    "    for path in cls_files:\n",
    "        df = pd.read_csv(path)\n",
    "        y_true  = df[\"y_true\"].astype(int).values\n",
    "        y_score = df[\"y_score\"].astype(float).values\n",
    "        if np.unique(y_true).size < 2:\n",
    "            # degenerate split; skip so it doesn't skew means\n",
    "            continue\n",
    "\n",
    "        # MATCH ratios convention: raw fpr/tpr, no dedup, no forcing tpr[-1]=1.0\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "\n",
    "        # Interpolate to common grid for plotting only\n",
    "        tpr_i = np.interp(fpr_grid, fpr, tpr)\n",
    "        tpr_i[0] = 0.0\n",
    "        tprs_interp.append(tpr_i)\n",
    "\n",
    "        # Scalar AUC from raw fpr/tpr\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "\n",
    "    if not tprs_interp:\n",
    "        continue\n",
    "\n",
    "    mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "    std_tpr  = np.std(tprs_interp, axis=0)\n",
    "    mean_auc = float(np.mean(aucs))\n",
    "    std_auc  = float(np.std(aucs))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr_grid, mean_tpr, lw=2,\n",
    "        label=f\"{cls.replace('_',' ')} (AUC = {mean_auc:.3f} ± {std_auc:.3f})\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        fpr_grid,\n",
    "        np.clip(mean_tpr - std_tpr, 0, 1),\n",
    "        np.clip(mean_tpr + std_tpr, 0, 1),\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "# Chance line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('RF (Proteomics + Metadata)')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — TRAINING\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---- Output folder ----\n",
    "out_dir = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---- Helper: group-aware stratified 70/30 split (by mode of strata per subject) ----\n",
    "def group_stratified_shuffle_split(df_index, strata_all, groups_all, test_size=0.30, random_state=0):\n",
    "    \"\"\"Return train_idx, test_idx ensuring groups stay intact and class-sex balance is approx preserved.\n",
    "       Strategy: assign each group a single stratum = mode(strata) among its rows, then stratified split at group level.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    data = pd.DataFrame({\"idx\": df_index, \"strata\": strata_all, \"group\": groups_all})\n",
    "\n",
    "    # group -> mode(strata)\n",
    "    grp_mode = (\n",
    "        data.groupby(\"group\")[\"strata\"]\n",
    "        .agg(lambda s: s.value_counts().idxmax())\n",
    "    )\n",
    "    grp_mode = grp_mode.sample(frac=1.0, random_state=random_state)  # shuffle groups\n",
    "\n",
    "    train_groups, test_groups = [], []\n",
    "    for s_val, grp_ids in grp_mode.groupby(grp_mode.values):\n",
    "        g_list = list(grp_ids.index)\n",
    "        rng.shuffle(g_list)\n",
    "        n_test = max(1, int(round(test_size * len(g_list))))\n",
    "        test_groups.extend(g_list[:n_test])\n",
    "        train_groups.extend(g_list[n_test:])\n",
    "\n",
    "    train_mask = np.isin(groups_all, train_groups)\n",
    "    test_mask  = np.isin(groups_all,  test_groups)\n",
    "    return np.where(train_mask)[0], np.where(test_mask)[0]\n",
    "\n",
    "# ---- Classes & seeds ----\n",
    "classes = [\"MCI\",\"NCI\",\"AD\",\"AD+\"]     # will save AD+ as 'ADplus' in filenames\n",
    "def safe_cls(c): return c.replace(\"+\",\"plus\").replace(\" \",\"_\").replace(\"/\",\"-\")\n",
    "\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "# ---- Main loop ----\n",
    "for seed in seeds:\n",
    "    tr_idx, te_idx = group_stratified_shuffle_split(\n",
    "        df_index=np.arange(len(X)),\n",
    "        strata_all=strata,\n",
    "        groups_all=groups,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_train_full    = y[tr_idx]\n",
    "    y_test_full     = y[te_idx]\n",
    "\n",
    "    print(f\"\\n[Seed {seed}] Train n={len(tr_idx)} | Test n={len(te_idx)} | \"\n",
    "          f\"Groups train={len(np.unique(groups[tr_idx]))} test={len(np.unique(groups[te_idx]))}\")\n",
    "\n",
    "    for cls in classes:\n",
    "        y_train = (y_train_full == cls).astype(int)\n",
    "        y_test  = (y_test_full  == cls).astype(int)\n",
    "\n",
    "        automl = AutoML()\n",
    "        settings = {\n",
    "            \"time_budget\": 100,               # seconds per class; adjust if you want longer searches\n",
    "            \"metric\": \"roc_auc\",\n",
    "            \"task\": \"classification\",\n",
    "            \"eval_method\": \"cv\",\n",
    "            \"estimator_list\": [\"lgbm\"],       # force LightGBM\n",
    "            \"log_file_name\": os.path.join(out_dir, f\"flaml_seed{seed}_{safe_cls(cls)}.log\"),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "        automl.pickle(os.path.join(out_dir, f\"seed{seed}_{safe_cls(cls)}_automl.pkl\"))\n",
    "\n",
    "        # Predict proba for positive class\n",
    "        y_score = automl.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_score) if (y_test.sum() > 0 and y_test.sum() < len(y_test)) else float(\"nan\")\n",
    "        print(f\"  [{cls}] AUC={auc:.3f}\")\n",
    "\n",
    "        # Save per-class CSV for your existing averaging/plotting script\n",
    "        out_csv = os.path.join(out_dir, f\"seed{seed}_{safe_cls(cls)}.csv\")\n",
    "        pd.DataFrame({\"y_true\": y_test.astype(int), \"y_score\": y_score.astype(float)}).to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ---- Config ----\n",
    "out_dir = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\"\n",
    "classes = [\"MCI\", \"NCI\", \"AD\", \"AD+\"]\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "def safe_cls(c): return c.replace(\"+\", \"plus\").replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "# ---- Track all features with non-zero importance ----\n",
    "feature_counts = Counter()\n",
    "\n",
    "for seed in seeds:\n",
    "    for cls in classes:\n",
    "        model_path = os.path.join(out_dir, f\"seed{seed}_{safe_cls(cls)}_automl.pkl\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Missing: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            automl = pickle.load(f)\n",
    "\n",
    "        # Get feature importances\n",
    "        importances = automl.model.estimator.feature_importances_\n",
    "        features = automl.feature_names_in_\n",
    "\n",
    "        # Filter to non-zero importance\n",
    "        nonzero_features = [feat for feat, imp in zip(features, importances) if imp > 0]\n",
    "\n",
    "        # Update count for each non-zero feature\n",
    "        feature_counts.update(nonzero_features)\n",
    "\n",
    "# ---- Filter to features appearing in at least 3 models ----\n",
    "shared_features = [feat for feat, count in feature_counts.items() if count >= 3]\n",
    "print(f\"\\n Found {len(shared_features)} features with non-zero importance in ≥3 models.\\n\")\n",
    "print(shared_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ---- Update this to match your proteomics run ----\n",
    "folder = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\"\n",
    "files = glob.glob(os.path.join(folder, \"seed*_*.csv\"))\n",
    "\n",
    "# Infer class names from filenames (everything after 'seedX_')\n",
    "def class_from_path(p):\n",
    "    base = os.path.basename(p)\n",
    "    return \"_\".join(base.split(\"_\")[1:]).rsplit(\".csv\", 1)[0]\n",
    "\n",
    "class_names = sorted(set(class_from_path(p) for p in files))\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cls in class_names:\n",
    "    cls_files = sorted(glob.glob(os.path.join(folder, f\"seed*_{cls}.csv\")))\n",
    "    if not cls_files:\n",
    "        continue\n",
    "\n",
    "    tprs_interp, aucs = [], []\n",
    "\n",
    "    for path in cls_files:\n",
    "        df = pd.read_csv(path)\n",
    "        y_true  = df[\"y_true\"].astype(int).values\n",
    "        y_score = df[\"y_score\"].astype(float).values\n",
    "\n",
    "        # EXACTLY like your ratios plot: use raw fpr/tpr directly\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "\n",
    "        # Interpolate to common grid (no FPR de-dup)\n",
    "        tpr_i = np.interp(fpr_grid, fpr, tpr)\n",
    "        tpr_i[0] = 0.0  # match your ratios script\n",
    "        tprs_interp.append(tpr_i)\n",
    "\n",
    "        # AUC computed on raw fpr/tpr (no de-dup)\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "\n",
    "    if not tprs_interp:\n",
    "        continue\n",
    "\n",
    "    mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "    std_tpr  = np.std(tprs_interp, axis=0)\n",
    "    mean_auc = float(np.mean(aucs))\n",
    "    std_auc  = float(np.std(aucs))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr_grid, mean_tpr, lw=2,\n",
    "        label=f\"{cls.replace('_',' ')} (AUC = {mean_auc:.3f} ± {std_auc:.3f})\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        fpr_grid,\n",
    "        np.clip(mean_tpr - std_tpr, 0, 1),\n",
    "        np.clip(mean_tpr + std_tpr, 0, 1),\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "# Chance line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LGBM (Proteomics + Metadata)')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ---- Update this to match your proteomics run ----\n",
    "folder = \"/Users/authorname/Desktop/Projects/proteomics_LGBM(ANML+Meta)\"\n",
    "files = glob.glob(os.path.join(folder, \"seed*_*.csv\"))\n",
    "\n",
    "# Infer class names from filenames (everything after 'seedX_')\n",
    "def class_from_path(p):\n",
    "    base = os.path.basename(p)\n",
    "    return \"_\".join(base.split(\"_\")[1:]).rsplit(\".csv\", 1)[0]\n",
    "\n",
    "class_names = sorted(set(class_from_path(p) for p in files))\n",
    "\n",
    "recall_grid = np.linspace(0.0, 1.0, 200)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "ap_summary_rows = []\n",
    "\n",
    "for cls in class_names:\n",
    "    cls_files = sorted(glob.glob(os.path.join(folder, f\"seed*_{cls}.csv\")))\n",
    "    if not cls_files:\n",
    "        continue\n",
    "\n",
    "    prec_interp_list, aps, prevalences = [], [], []\n",
    "\n",
    "    for path in cls_files:\n",
    "        df = pd.read_csv(path)\n",
    "        y_true  = df[\"y_true\"].astype(int).values\n",
    "        y_score = df[\"y_score\"].astype(float).values\n",
    "\n",
    "        # skip degenerate splits\n",
    "        pos = y_true.sum()\n",
    "        if pos == 0 or pos == len(y_true):\n",
    "            continue\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "        # deduplicate recall for interpolation\n",
    "        uniq_idx = np.unique(recall, return_index=True)[1]\n",
    "        r_u, p_u = recall[uniq_idx], precision[uniq_idx]\n",
    "\n",
    "        # interpolate precision onto common recall grid\n",
    "        p_i = np.interp(recall_grid, r_u, p_u, left=p_u[0], right=p_u[-1])\n",
    "        prec_interp_list.append(p_i)\n",
    "\n",
    "        aps.append(average_precision_score(y_true, y_score))\n",
    "        prevalences.append(y_true.mean())\n",
    "\n",
    "    if not prec_interp_list:\n",
    "        print(f\"[WARN] No valid PR curves for class {cls}\")\n",
    "        continue\n",
    "\n",
    "    mean_prec = np.mean(prec_interp_list, axis=0)\n",
    "    std_prec  = np.std(prec_interp_list, axis=0)\n",
    "    mean_ap   = float(np.mean(aps))\n",
    "    std_ap    = float(np.std(aps))\n",
    "    n_splits  = len(aps)\n",
    "\n",
    "    label_cls = cls.replace(\"_\", \" \")\n",
    "    line, = plt.plot(recall_grid, mean_prec, lw=2,\n",
    "                     label=f\"{label_cls} (AP = {mean_ap:.3f} ± {std_ap:.3f})\")\n",
    "    plt.fill_between(recall_grid,\n",
    "                     np.clip(mean_prec - std_prec, 0, 1),\n",
    "                     np.clip(mean_prec + std_prec, 0, 1),\n",
    "                     alpha=0.15, color=line.get_color())\n",
    "\n",
    "    # per-class chance baseline = prevalence\n",
    "    prev_mean = float(np.mean(prevalences))\n",
    "    prev_sd   = float(np.std(prevalences))\n",
    "    plt.hlines(prev_mean, 0, 1, colors=line.get_color(), linestyles=\"--\", alpha=0.6)\n",
    "    low, high = max(0, prev_mean - prev_sd), min(1, prev_mean + prev_sd)\n",
    "    plt.fill_between([0, 1], [low, low], [high, high], color=line.get_color(), alpha=0.08)\n",
    "\n",
    "    ap_summary_rows.append({\n",
    "        \"Class\": label_cls, \"Mean_AP\": mean_ap, \"Std_AP\": std_ap,\n",
    "        \"Prev_Mean\": prev_mean, \"Prev_SD\": prev_sd, \"N\": n_splits\n",
    "    })\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"LGBM (Proteomics + Demographics)\\nPrecision–Recall (mean ± SD across seeds; dashed = prevalence)\")\n",
    "plt.legend(loc=\"upper right\", fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: save figure + table\n",
    "plot_path = os.path.join(folder, \"pr_mean_across_seeds.png\")\n",
    "plt.savefig(plot_path, dpi=200)\n",
    "if ap_summary_rows:\n",
    "    ap_df = pd.DataFrame(ap_summary_rows).sort_values(\"Class\")\n",
    "    ap_csv = os.path.join(folder, \"pr_ap_summary.csv\")\n",
    "    ap_df.to_csv(ap_csv, index=False)\n",
    "    print(f\"Saved plot to: {plot_path}\")\n",
    "    print(f\"Saved AP summary to: {ap_csv}\")\n",
    "    print(ap_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neuro_240)",
   "language": "python",
   "name": "neuro_240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
